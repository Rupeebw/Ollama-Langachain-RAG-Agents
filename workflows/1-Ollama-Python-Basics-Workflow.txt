================================================================================
                    OLLAMA PYTHON BASICS - ASCII WORKFLOW
================================================================================

╔══════════════════════════════════════════════════════════════════════════════╗
║                         1. INSTALLATION & SETUP                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

    ┌─────────────────┐
    │  pip install    │
    │     ollama      │
    └────────┬────────┘
             │
             ▼
    ┌─────────────────┐
    │ import ollama   │
    └─────────────────┘


╔══════════════════════════════════════════════════════════════════════════════╗
║                         2. DOWNLOADING MODELS                                ║
╚══════════════════════════════════════════════════════════════════════════════╝

    ┌──────────────────────────┐
    │ ollama.pull('llama3.1')  │
    └────────────┬─────────────┘
                 │
                 ▼
    ┌──────────────────────────┐
    │  Model Downloaded to     │
    │  Local Machine           │
    └──────────────────────────┘


╔══════════════════════════════════════════════════════════════════════════════╗
║                    3. BASIC INTERACTION METHODS                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

METHOD A: ollama.generate()
━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ┌─────────────────────────┐
    │  User Prompt            │
    │ "Tell me a joke"        │
    └──────────┬──────────────┘
               │
               ▼
    ┌──────────────────────────────────┐
    │ ollama.generate(                 │
    │   model='llama3.1:8b',          │
    │   prompt='Tell me a joke'       │
    │ )                                │
    └──────────┬───────────────────────┘
               │
               ▼
    ┌──────────────────────────┐
    │  Returns:                │
    │  - response              │
    │  - model info            │
    │  - tokens used           │
    │  - timing data           │
    └──────────────────────────┘


METHOD B: ollama.chat()
━━━━━━━━━━━━━━━━━━━━━━━
    ┌─────────────────────────┐
    │  Conversation Messages  │
    │  [                      │
    │    {role: 'user',       │
    │     content: '...'}     │
    │  ]                      │
    └──────────┬──────────────┘
               │
               ▼
    ┌──────────────────────────────────┐
    │ ollama.chat(                     │
    │   model='llama3.1:8b',          │
    │   messages=[...]                │
    │ )                                │
    └──────────┬───────────────────────┘
               │
               ▼
    ┌──────────────────────────┐
    │  Returns:                │
    │  - message               │
    │  - role                  │
    │  - metadata              │
    └──────────────────────────┘


╔══════════════════════════════════════════════════════════════════════════════╗
║                      4. CREATING CUSTOM MODELS                               ║
╚══════════════════════════════════════════════════════════════════════════════╝

    ┌─────────────────────────────────┐
    │  Define Modelfile               │
    │  ┌───────────────────────────┐  │
    │  │ FROM llama3.1:8b          │  │
    │  │ SYSTEM You are Jarvis...  │  │
    │  └───────────────────────────┘  │
    └──────────────┬──────────────────┘
                   │
                   ▼
    ┌──────────────────────────────────┐
    │ ollama.create(                   │
    │   model='jarvis',                │
    │   modelfile=modelfile            │
    │ )                                │
    └──────────────┬───────────────────┘
                   │
                   ▼
    ┌──────────────────────────────────┐
    │  Custom Model Created            │
    │  - Base: llama3.1:8b             │
    │  - Personality: Jarvis           │
    │  - Behavior: Custom system prompt│
    └──────────────────────────────────┘


╔══════════════════════════════════════════════════════════════════════════════╗
║                      5. MODEL MANAGEMENT                                     ║
╚══════════════════════════════════════════════════════════════════════════════╝

LIST MODELS
━━━━━━━━━━━━━━━━━━━━━━
    ┌─────────────────┐
    │ ollama.list()   │
    └────────┬────────┘
             │
             ▼
    ┌──────────────────────────┐
    │  Returns:                │
    │  - llama3.1:8b           │
    │  - jarvis:latest         │
    │  - deepseek-r1:1.5b      │
    │  (with size, date, etc.) │
    └──────────────────────────┘


DELETE MODEL
━━━━━━━━━━━━━━━━━━━━━━
    ┌─────────────────────────┐
    │ ollama.delete('jarvis') │
    └────────┬────────────────┘
             │
             ▼
    ┌──────────────────────────┐
    │  Model Removed from      │
    │  Local Storage           │
    └──────────────────────────┘


╔══════════════════════════════════════════════════════════════════════════════╗
║                      6. OLLAMA REST API CLIENT                               ║
╚══════════════════════════════════════════════════════════════════════════════╝

    ┌─────────────────────────────────┐
    │ from ollama import Client       │
    └──────────────┬──────────────────┘
                   │
                   ▼
    ┌─────────────────────────────────┐
    │ client = Client(                │
    │   host='http://localhost:11434' │
    │ )                               │
    └──────────────┬──────────────────┘
                   │
                   ▼
    ┌─────────────────────────────────┐
    │ client.chat(                    │
    │   model='llama3.1:8b',         │
    │   messages=[{...}]             │
    │ )                               │
    └─────────────────────────────────┘

    Benefits:
    ✓ More control over connection
    ✓ Custom host configuration
    ✓ Connection pooling
    ✓ Better for production use


╔══════════════════════════════════════════════════════════════════════════════╗
║                      7. OPENAI COMPATIBILITY                                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

    ┌─────────────────────────────────┐
    │ from openai import OpenAI       │
    └──────────────┬──────────────────┘
                   │
                   ▼
    ┌─────────────────────────────────┐
    │ llm = OpenAI(                   │
    │   base_url='http://localhost:   │
    │             11434/v1',          │
    │   api_key='blank'               │
    │ )                               │
    └──────────────┬──────────────────┘
                   │
                   ▼
    ┌─────────────────────────────────┐
    │ llm.chat.completions.create(    │
    │   model='llama3.1:8b',         │
    │   messages=[{...}]             │
    │ )                               │
    └─────────────────────────────────┘

    Use Cases:
    ✓ Drop-in replacement for OpenAI
    ✓ Local/private deployment
    ✓ No API costs
    ✓ Compatible with OpenAI tools


╔══════════════════════════════════════════════════════════════════════════════╗
║                         COMPLETE WORKFLOW                                    ║
╚══════════════════════════════════════════════════════════════════════════════╝

    START
      │
      ▼
    ┌─────────────────────┐
    │ Install Ollama      │
    │ pip install ollama  │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ Download Model      │
    │ ollama.pull()       │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────────────────────┐
    │ Choose Interaction Method:          │
    │                                     │
    │  A. Simple: ollama.generate()       │
    │  B. Chat: ollama.chat()             │
    │  C. REST API: Client.chat()         │
    │  D. OpenAI Compatible: OpenAI()     │
    └──────────┬──────────────────────────┘
               │
               ▼
    ┌─────────────────────┐
    │ Send Prompt/Message │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ Receive Response    │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ Process Output      │
    └──────────┬──────────┘
               │
               ▼
             END


╔══════════════════════════════════════════════════════════════════════════════╗
║                         KEY CONCEPTS                                         ║
╚══════════════════════════════════════════════════════════════════════════════╝

┌────────────────────────────────────────────────────────────────────────────┐
│ 1. LOCAL EXECUTION                                                         │
│    - Models run on your machine                                            │
│    - No internet required after download                                   │
│    - Full privacy and control                                              │
└────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────────────────────┐
│ 2. MODEL FORMATS                                                           │
│    - model_name:size (e.g., llama3.1:8b)                                  │
│    - Different quantizations available                                     │
│    - Trade-off: size vs accuracy                                          │
└────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────────────────────┐
│ 3. CUSTOMIZATION                                                           │
│    - Create custom models with Modelfile                                   │
│    - Define system prompts                                                 │
│    - Set parameters (temperature, context size, etc.)                      │
└────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────────────────────┐
│ 4. API COMPATIBILITY                                                       │
│    - Works with OpenAI SDK                                                 │
│    - Easy migration from cloud to local                                    │
│    - Compatible with existing tools                                        │
└────────────────────────────────────────────────────────────────────────────┘


================================================================================
                              END OF WORKFLOW
================================================================================
