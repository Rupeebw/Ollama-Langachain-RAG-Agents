{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RAG Application using Ollama and LangChain\n",
    "\n",
    "This notebook demonstrates building a Retrieval-Augmented Generation (RAG) application using:\n",
    "- LangChain for orchestration\n",
    "- Ollama for local LLM inference\n",
    "- ChromaDB for vector storage\n",
    "- Text embeddings for semantic search\n",
    "\n",
    "## What is RAG?\n",
    "RAG combines the capabilities of large language models with external knowledge retrieval, allowing AI systems to access up-to-date information beyond their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install langchain-core\n",
    "!pip3 install langchain-ollama\n",
    "!pip3 install langchain_community\n",
    "!pip3 install langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langchain-basics",
   "metadata": {},
   "source": [
    "## LangChain Basics with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef305b-3c2b-49f4-911a-e09ccf5bed81",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47cc00-0a8e-41b6-9e09-e684f9aa7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that gives a one-line definition of the word entered by user\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(user_input=\"Sesquipedalian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-ollama",
   "metadata": {},
   "source": [
    "### ChatOllama Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ca89d-728d-47d5-9f98-fb5108947a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b8650-b760-41fd-aa7b-be394440b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chains",
   "metadata": {},
   "source": [
    "### Creating Chains with Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be71b0e-e80d-4a6f-86e6-cb26b5cd513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = chat_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3a7d5-2e67-4d56-9b4f-eb61b5092d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"user_input\": \"granny\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4a27e-1258-4cf8-9730-6f9cad7c7903",
   "metadata": {},
   "source": [
    "## Building a RAG Application\n",
    "\n",
    "Now let's build a complete RAG pipeline with the following components:\n",
    "1. Document Loader\n",
    "2. Text Splitter\n",
    "3. Embeddings\n",
    "4. Vector Store (ChromaDB)\n",
    "5. Retriever\n",
    "6. RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "### Import Required Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2054a8-07c3-4481-b4ff-60ec7bd7a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "### Step 1: Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8cd01-e4cd-4863-85fc-f2122b4ac828",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = TextLoader(\"./LangchainRetrieval.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d737a-3aab-44b0-821b-ce89a8a6dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "### Step 2: Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863d426-7535-46d8-b129-ca89eaed45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcbb7c-be62-42b2-8a94-88094da21053",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435a3c5-06b8-48f8-8862-f207dbb26acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "### Step 3: Create Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56188627-136a-4983-9aee-c1e4c0eba147",
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "### Step 4: Create Vector Store (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2fbb7-48fa-49e4-a645-cb7ef40f0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents, embedding=oembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-search",
   "metadata": {},
   "source": [
    "### Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a1b75-0a98-4352-abc0-ffe4d1d05a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is text embedding and how does langchain help in doing it\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f756e-f7e4-4585-91d6-6a7b85b57d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca25a4e-8857-48b6-a76f-4d974e3ffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[3].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "### Step 5: Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178f60d-97bf-41d2-be40-b9bae13407fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e90123-36fc-45e5-b43c-5c30d0e6f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11573e3-fc4c-435c-895f-1b5d853445ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b7084-2cc7-4793-b9a5-bda584caacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe830d-206a-4696-b969-beb8ab0a5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b73779-543c-46fd-9599-1bece7c14893",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "### Step 6: Use the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e6d8d-b1a5-4084-ba43-bbbabc76eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is text embedding and how does langchain help in doing it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've built a complete RAG application that:\n",
    "1. Loads documents from text files\n",
    "2. Splits them into manageable chunks\n",
    "3. Creates embeddings using Ollama's nomic-embed-text model\n",
    "4. Stores embeddings in ChromaDB vector database\n",
    "5. Retrieves relevant context based on user queries\n",
    "6. Generates accurate answers using the Llama 3.1 model\n",
    "\n",
    "This approach allows the LLM to answer questions based on your custom documents rather than relying solely on its training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
